# -*- coding: utf-8 -*-

from __future__ import unicode_literals
import os, sys
from os.path import join
from collections import defaultdict
from nltk.stem.snowball import RussianStemmer
from nltk.stem.snowball import GermanStemmer
from porter import PorterStemmer
import codecs, re, json
import pymorphy2


# –°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è –æ—Å–Ω–æ–≤ —Å–ª–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π –∏—Ö –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
# –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∫–æ—Ä–ø—É—Å–æ–≤. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ 
# –≤—ã—Ö–æ–¥–Ω–æ–π json –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: 1) tfidf - –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–¥—Å—á–µ—Ç–∞
# tf-idf, 2) raw - –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã –æ—Å–Ω–æ–≤—ã –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.
# –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–Ω–æ–≤—É –∏ —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ —Å–∫–æ–ª—å–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
# –∫–æ—Ä–ø—É—Å–∞ –æ—Å–Ω–æ–≤–∞ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å (–∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É). 
# {"football":2} –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –æ—Å–Ω–æ–≤–∞ football –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å –≤ –¥–≤—É—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–∑ –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞.
# 
# –°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç—Ä—ë—Ö —è–∑—ã–∫–æ–≤: –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, –Ω–µ–º–µ—Ü–∫–æ–≥–æ, —Ä—É—Å—Å–∫–æ–≥–æ.
# –ö–ª–∞—Å—Å LoadExternalLists –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã,
# –∫–ª–∞—Å—Å NormalizerDE - –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ–º–µ—Ü–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤,
# –∫–ª–∞—Å—Å NormalizerRU - —Ç–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ë –≤ –µ,
# –∫–ª–∞—Å—Å NormalizerEN - —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º.
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥—É–ª—è Porter Stemmer,
# pymorphy2 –∏ nltk.


class LoadExternalLists(object):
    
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ø–∞–º—è—Ç—å —Ñ–∞–π–ª —Å—Ç–æ–ø-—Å–ª–æ–≤, —Ñ–∞–π–ª —Å –Ω–µ–ø—Ä–∞–≤. —Ñ–æ—Ä–º–∞–º–∏ –≥–ª–∞–≥–æ–ª–æ–≤ 
    –∏ —Ñ–∞–π–ª —Å –Ω–µ–ø—Ä–∞–≤. —Ñ–æ—Ä–º–∞–º–∏ –º–Ω–æ–∂. —á–∏—Å–ª–∞ —Å—É—â-—Ö.
    """

    def __init__(self):

        self.stopwords_en = set()
        self.stopwords_de = set()
        self.stopwords_ru = set()
        
        self.verbtransforms = {}
        self.nountransforms = {}

        self.lexicon_de = {}
        
        
    def loadStopWordsEN(self):

        with codecs.open(r".\txt_resources\stopwords_en.txt",'r','utf-16') as file_openstopw:
        	self.stopwords = set(file_openstopw.read().split('\r\n'))

        return self.stopwords


    def loadStopWordsDE(self):

        with codecs.open(r".\txt_resources\stopwords_de.txt",'r','utf-16') as file_openstopw:
        	self.stopwords = set(file_openstopw.read().split('\r\n'))

        return self.stopwords


    def loadStopWordsRU(self):

        with codecs.open(r".\txt_resources\stopwords_ru.txt",'r','utf-16') as file_openstopw:
        	self.stopwords = set(file_openstopw.read().split('\r\n'))

        return self.stopwords


    def loadVerbForms(self):

        with codecs.open(r'.\txt_resources\verbforms.txt','r','utf-16') as verbforms:
	        for line in verbforms:
	            line_part = line.strip().split('\t')
	            if line_part[0] not in self.verbtransforms:
	                self.verbtransforms[line_part[0]] = line_part[1]

        return self.verbtransforms

    def loadNounforms(self):

        with codecs.open(r'.\txt_resources\nounforms.txt','r','utf-16') as nounforms:
	        for line in nounforms:
	            line_part = line.strip().split('\t')
	            if line_part[0] not in self.nountransforms:
	                self.nountransforms[line_part[0]] = line_part[1]

        return self.nountransforms

    def loadLexiconDe(self):

        with open(r'.\lexicon\lexicon_dict_49289.json', 'r') as infile:

            self.lexicon_de = json.load(infile)

        return self.lexicon_de


class NormalizerDE(object):

	
	def normalizeUmlaut(self, word):
		"""
		–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–º–µ–∫—Ü–∏–µ —É–º–ª–∞—É—Ç—ã –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏. –û–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–µ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–æ.
		"""

		# cyr_err = {'–¥':'√§', '—Ü':'√∂', '—å':'√º', '–Ø':'√ü', '–î':'√Ñ', '–¶':'√ñ', '–¨':'√ú'}
		# umlauts_case = {'√§':'ae', '√∂':'oe', '√º':'ue', '√ü':'ss', '√Ñ':'Ae', '√ñ':'Oe', '√ú':'ue', '–¥':'ae', '—Ü':'oe', '—å':'ue', '–Ø':'ss', '–î':'Ae', '–¶':'Oe', '–¨':'Ue'}
		umlauts = {'√§':'ae', '√∂':'oe', '√º':'ue', '√ü':'ss', '–¥':'ae', '—Ü':'oe', '—å':'ue', '–Ø':'ss', 'aÃà':'ae', 'oÃà':'oe', 'uÃà':'ue'}

		for umlaut, ersatz in umlauts.iteritems():
			if umlaut in word:
				word = word.replace(umlaut, ersatz)
				
		return word


	def deleteContrs(self, str1):
		"""
		–ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤ –ø—É—Ç–µ–º –º–µ—Ç–æ–¥–∞ re.sub (–∑–∞–º–µ–Ω—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ –Ω–∞ –Ω–∏—á—Ç–æ).
		–ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤—Å—ë –µ—â—ë –Ω–∞–π–¥–µ–Ω —É —Å–ª–æ–≤–∞ (–∞–Ω–≥–ª: I'd've), —Ç–æ –µ—â—ë —Ä–∞–∑ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ re.sub.
		"""

		# –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–∏–ø–∞ you've, don't –∏ –ø—Ä.
		del_endings = re.compile(r'[\'\‚Äô`‚Äò]+[s|m|t|d|n]$|[\'\‚Äô‚Äò`]+(ve|ll|re|nt|ya|yer)$')

		no_endings = del_endings.sub('', str1)

		if del_endings.search(no_endings):

			no_endings2 = del_endings.sub('', no_endings)
			
			return no_endings2
		else:

			return no_endings


	def lemmatize(self, word, lexicon):
		"""
		–§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–µ–º–µ—Ü–∫–∏—Ö —Å–ª–æ–≤.
		–ü–æ–¥–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ—Å–æ–±—ã–º –æ–±—Ä–∞–∑–æ–º —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ª–µ–∫—Å–∏–∫–æ–Ω.
		–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ª–µ–∫—Å–∏–∫–æ–Ω–∞: –ø–∏—Ç–æ–Ω–æ–≤—Å–∫–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏.
		–ü–µ—Ä–≤—ã–π —É—Ä–æ–≤–µ–Ω—å - –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –±—É–∫–≤—ã —Å–ª–æ–≤–∞ (—Å–º. –ø–µ—Ä–µ–º. alphabet).
		–í–Ω—É—Ç—Ä–∏ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å–ª–æ–≤–∞, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–∞ —ç—Ç—É –±—É–∫–≤—É, –∫–ª—é—á - –ª–µ–º–º–∞,
		–∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º —ç—Ç–æ–π –ª–µ–º–º—ã. –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ—Ö –±—É–∫–≤ —Å–¥–µ–ª–∞–Ω–æ
		–¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∫–∞–∫-—Ç–æ —É–º–µ–Ω—å—à–∏—Ç—å –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã. –¢–∞–∫ –Ω–µ –Ω–∞–¥–æ
		–ø—Ä–æ–±–µ–≥–∞—Ç—å –ø–æ –≤—Å–µ–º—É —Å–ª–æ–≤–∞—Ä—é, –∞ –∏—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –ø–æ–¥—Å–ª–æ–≤–∞—Ä–µ.
		–ü–æ–∏—Å–∫:
		–ü–æ–∏—Å–∫ –∏–¥—ë—Ç –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤–æ—Ñ–æ—Ä–º, –ø–æ–∫–∞ –Ω–µ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—Å—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ñ–æ—Ä–º–∞.
		–°–Ω–∞—á–∞–ª–∞ —Å–ª–æ–≤–æ –∏—â–µ—Ç—Å—è —Ü–µ–ª–∏–∫–æ–º, –µ—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è, –æ—Ç–∫—É—Å—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–≤–∞—è
		–±—É–∫–≤–∞ —Å–ª–µ–≤–∞ –∏ —Ç–∞–∫ –¥–æ –∫–æ–Ω—Ü–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ 
		—Å–ª–æ–≤–∞ —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ (ueberbakterien) –∏ —Å–ª–æ–≤–∞-–∫–æ–º–ø–∞—É–Ω–¥—ã (krankenversicherungsbescheinigungen),
		–∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ ktrcbrjyt, –Ω–æ –µ—Å—Ç—å –∏—Ö –ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å (bescheinigungen).
		"""

		alphabet = tuple(["aac", "aal", "aas", "aba", "abb", "abc", "abd", "abe", "abf", "abg", "abh", "abi", "abj", "abk", "abl", "abm", "abn", "abo", "abp", "abq", "abr", "abs", "abt", "abu", "abv", "abw", "abz", "acc", "ach", "ack", "act", "ada", "add", "ade", "adh", "adj", "adl", "adm", "ado", "adr", "ads", "adv", "aeb", "aec", "aed", "aef", "aeg", "aeh", "ael", "aem", "aen", "aep", "aeq", "aer", "aes", "aet", "aeu", "aex", "aff", "afg", "afr", "aft", "aga", "age", "agg", "agi", "ago", "agr", "ags", "ahl", "ahm", "ahn", "aho", "air", "aka", "akk", "akn", "ako", "akq", "akr", "akt", "aku", "akw", "akz", "ala", "alb", "alc", "ale", "alf", "alg", "ali", "alk", "all", "alm", "alp", "alr", "als", "alt", "alu", "alz", "ama", "amb", "ame", "amh", "ami", "amm", "amn", "amo", "amp", "ams", "amt", "amu", "ana", "anb", "and", "ane", "anf", "ang", "anh", "ani", "anj", "ank", "anl", "anm", "ann", "ano", "anp", "anq", "anr", "ans", "ant", "anv", "anw", "anz", "aor", "apa", "ape", "apf", "aph", "apo", "app", "apr", "aps", "aqu", "ara", "arb", "arc", "ard", "are", "arg", "arh", "ari", "ark", "arm", "arn", "aro", "arr", "ars", "art", "arz", "asb", "asc", "ase", "asi", "ask", "aso", "asp", "ass", "ast", "asy", "asz", "ate", "ath", "atl", "atm", "ato", "atr", "att", "atu", "aub", "aud", "aue", "auf", "aug", "auh", "auk", "aul", "aup", "aur", "aus", "aut", "ava", "ave", "avi", "axe", "axi", "aza", "aze", "azt", "azu", "bab", "bac", "bad", "bae", "baf", "bag", "bah", "bai", "baj", "bak", "bal", "bam", "ban", "bap", "bar", "bas", "bat", "bau", "bay", "baz", "bea", "beb", "bec", "bed", "bee", "bef", "beg", "beh", "bei", "bej", "bek", "bel", "bem", "ben", "beo", "bep", "beq", "ber", "bes", "bet", "beu", "bev", "bew", "bez", "bia", "bib", "bie", "big", "bik", "bil", "bim", "bin", "bio", "bir", "bis", "bit", "biw", "biz", "bjo", "bla", "ble", "bli", "blo", "blu", "bmw", "boa", "bob", "boc", "bod", "boe", "bog", "boh", "boi", "boj", "bol", "bom", "bon", "boo", "bor", "bos", "bot", "bou", "bow", "box", "boy", "bra", "bre", "bri", "bro", "bru", "bub", "buc", "bud", "bue", "buf", "bug", "buh", "buk", "bul", "bum", "bun", "bur", "bus", "but", "byt", "byz", "cac", "cad", "cae", "caf", "cag", "cal", "cam", "can", "cap", "car", "cas", "cd-", "cds", "cea", "cel", "cem", "cen", "ceo", "cer", "ces", "cey", "cha", "che", "chi", "chl", "cho", "chr", "cie", "cin", "cit", "cla", "cle", "cli", "clo", "clu", "coa", "coc", "cod", "coh", "coi", "cok", "col", "com", "con", "coo", "cop", "cor", "cou", "cov", "cow", "coy", "cpu", "cra", "cre", "cro", "cto", "cup", "cur", "cut", "cya", "cyb", "cyr", "dab", "dac", "dad", "dae", "daf", "dag", "dah", "dai", "dak", "dal", "dam", "dan", "dar", "das", "dat", "dau", "dav", "daw", "daz", "dea", "deb", "dec", "ded", "dee", "def", "deg", "deh", "dei", "dej", "dek", "del", "dem", "den", "deo", "dep", "der", "des", "det", "deu", "dev", "dez", "di.", "dia", "dic", "did", "die", "dif", "dig", "dik", "dil", "dim", "din", "dio", "dip", "dir", "dis", "dit", "div", "diw", "djs", "do.", "doc", "dod", "doe", "dog", "doh", "dok", "dol", "dom", "don", "doo", "dop", "dor", "dos", "dot", "dou", "dow", "doz", "dra", "dre", "dri", "dro", "dru", "dsc", "dtu", "dua", "dub", "duc", "dud", "due", "duf", "dui", "dul", "dum", "dun", "duo", "dup", "dur", "dus", "dut", "duz", "dvd", "dyn", "dys", "ebb", "ebe", "ebn", "ech", "eck", "ecu", "edb", "edd", "ede", "edg", "edi", "edl", "edm", "edu", "efe", "eff", "ega", "ege", "egg", "ego", "ehe", "ehr", "eib", "eic", "eid", "eie", "eif", "eig", "eil", "eim", "ein", "eis", "eit", "eiw", "eiz", "eja", "eje", "eke", "ekl", "eks", "ekz", "ela", "elc", "eld", "ele", "elf", "eli", "elk", "ell", "elm", "elo", "els", "elt", "ema", "emb", "eme", "emi", "emm", "emo", "emp", "ems", "emu", "enc", "end", "ene", "enf", "eng", "enk", "eno", "ens", "ent", "enu", "enz", "epe", "epi", "epo", "epp", "eps", "equ", "era", "erb", "erc", "erd", "ere", "erf", "erg", "erh", "eri", "erj", "erk", "erl", "erm", "ern", "ero", "erp", "erq", "err", "ers", "ert", "eru", "erw", "erz", "esc", "ese", "esk", "eso", "esp", "ess", "est", "eta", "eth", "eti", "etu", "etw", "ety", "euc", "eug", "eul", "eun", "eup", "eur", "eut", "eva", "eve", "evi", "evo", "ewi", "exa", "exc", "exe", "exh", "exi", "exk", "exm", "exn", "exo", "exp", "exq", "exs", "ext", "exz", "eyl", "fab", "fac", "fad", "fae", "fag", "fah", "fai", "fak", "fal", "fam", "fan", "far", "fas", "fat", "fau", "fav", "fax", "faz", "fea", "feb", "fec", "fed", "fee", "feg", "feh", "fei", "fel", "fem", "fen", "fer", "fes", "fet", "feu", "fez", "fia", "fib", "fic", "fid", "fie", "fig", "fik", "fil", "fim", "fin", "fir", "fis", "fit", "fix", "fjo", "fla", "fle", "fli", "flo", "flu", "fly", "foc", "foe", "foh", "fok", "fol", "fon", "fop", "for", "fos", "fot", "fou", "foy", "fr.", "fra", "fre", "fri", "fro", "fru", "fuc", "fue", "fuf", "fug", "fuh", "ful", "fum", "fun", "fur", "fus", "fut", "g'f", "g'm", "g'n", "g'r", "g's", "g'w", "gab", "gad", "gae", "gaf", "gag", "gal", "gam", "gan", "gar", "gas", "gat", "gau", "gaz", "gbs", "gby", "gea", "geb", "gec", "ged", "gee", "gef", "geg", "geh", "gei", "gej", "gek", "gel", "gem", "gen", "geo", "gep", "geq", "ger", "ges", "get", "geu", "gev", "gew", "gez", "gha", "ghe", "ghu", "gib", "gie", "gif", "gig", "gil", "gim", "gin", "gip", "gir", "gis", "git", "giu", "gla", "gle", "gli", "glo", "glu", "gly", "gmb", "gna", "gne", "gno", "gnu", "gob", "goc", "god", "goe", "gog", "gol", "gom", "gon", "gor", "gos", "got", "gou", "gra", "gre", "gri", "gro", "gru", "gua", "guc", "gud", "gue", "gui", "gul", "gum", "gun", "gur", "gus", "gut", "gym", "gyn", "gys", "g‚Äòf", "g‚Äòm", "g‚Äòn", "g‚Äòr", "g‚Äòs", "g‚Äòw", "g‚Äôf", "g‚Äôm", "g‚Äôn", "g‚Äôr", "g‚Äôs", "g‚Äôw", "haa", "hab", "hac", "had", "hae", "haf", "hag", "hah", "hai", "hak", "hal", "ham", "han", "hap", "har", "has", "hat", "hau", "hav", "haw", "hax", "hay", "haz", "hea", "heb", "hec", "hed", "hee", "hef", "heg", "heh", "hei", "hek", "hel", "hem", "hen", "her", "hes", "het", "heu", "hex", "hey", "hic", "hie", "hig", "hil", "him", "hin", "hip", "hir", "his", "hit", "hiw", "hob", "hoc", "hod", "hoe", "hof", "hoh", "hok", "hol", "hom", "hon", "hoo", "hop", "hor", "hos", "hot", "hou", "hoy", "hub", "huc", "hue", "huf", "hug", "huh", "hul", "hum", "hun", "hup", "hur", "hus", "hut", "hya", "hyb", "hyd", "hye", "hyg", "hym", "hyp", "hys", "ibm", "ibr", "ico", "ics", "ide", "idi", "ido", "idy", "ige", "igl", "ign", "igo", "ike", "iko", "ilb", "ill", "ils", "ilt", "ima", "imb", "imi", "imk", "imm", "imp", "ina", "inb", "inc", "ind", "ine", "inf", "ing", "inh", "ini", "inj", "ink", "inl", "inn", "ino", "inq", "ins", "int", "inv", "inw", "inz", "iod", "ion", "iqs", "ira", "ird", "ire", "iri", "irl", "irm", "iro", "irr", "isa", "ise", "isl", "ism", "iso", "isr", "iss", "ist", "ita", "ite", "iva", "iza", "ize", "jac", "jae", "jag", "jah", "jak", "jal", "jam", "jan", "jap", "jar", "jas", "jau", "jaw", "jaz", "jea", "jec", "jed", "jee", "jel", "jen", "jer", "jes", "jet", "jew", "jim", "jin", "joa", "job", "joc", "jod", "joe", "jog", "joh", "joi", "jok", "jol", "jon", "jop", "jor", "jos", "jot", "jou", "jov", "jua", "jub", "juc", "jud", "jue", "jug", "jul", "jum", "jun", "jup", "jur", "jus", "jut", "juw", "jux", "jva", "kab", "kac", "kad", "kae", "kaf", "kah", "kai", "kaj", "kak", "kal", "kam", "kan", "kap", "kar", "kas", "kat", "kau", "kav", "kbi", "kbs", "kby", "keb", "kec", "kee", "kef", "keg", "keh", "kei", "kek", "kel", "kem", "ken", "ker", "kes", "ket", "keu", "kev", "kha", "kib", "kic", "kid", "kie", "kif", "kil", "kim", "kin", "kio", "kip", "kir", "kis", "kit", "kiw", "kla", "kle", "kli", "klo", "klu", "kna", "kne", "kni", "kno", "knu", "koa", "kob", "koc", "kod", "koe", "kof", "kog", "koh", "koi", "koj", "kok", "kol", "kom", "kon", "koo", "kop", "kor", "kos", "kot", "kra", "kre", "kri", "kro", "kru", "kry", "kub", "kuc", "kue", "kuf", "kug", "kuh", "kul", "kum", "kun", "kup", "kur", "kus", "kut", "kuv", "kuw", "kyb", "kyr", "kzs", "lab", "lac", "lad", "lae", "laf", "lag", "lah", "lai", "lak", "lal", "lam", "lan", "lap", "laq", "lar", "las", "lat", "lau", "lav", "law", "lax", "lay", "laz", "lea", "leb", "lec", "led", "lee", "leg", "leh", "lei", "lek", "lem", "len", "leo", "ler", "les", "let", "leu", "lev", "lex", "lia", "lib", "lic", "lid", "lie", "lif", "lig", "lii", "lik", "lil", "lim", "lin", "lip", "liq", "lis", "lit", "liv", "liz", "lkw", "lob", "loc", "lod", "loe", "lof", "log", "loh", "loi", "lok", "lol", "lom", "lon", "loo", "lor", "los", "lot", "lov", "loy", "lps", "luc", "lud", "lue", "luf", "lug", "luk", "lul", "lum", "lun", "lup", "lur", "lus", "lut", "lux", "luz", "lyk", "lym", "lyn", "lyr", "lys", "maa", "mac", "mad", "mae", "maf", "mag", "mah", "mai", "maj", "mak", "mal", "mam", "man", "map", "mar", "mas", "mat", "mau", "max", "may", "maz", "mbi", "mbs", "mby", "mec", "med", "mee", "meg", "meh", "mei", "mek", "mel", "mem", "men", "mep", "mer", "mes", "met", "meu", "mex", "mey", "mez", "mi.", "mia", "mic", "mie", "mig", "mih", "mik", "mil", "mim", "min", "mir", "mis", "mit", "mix", "mne", "mo.", "mob", "moc", "mod", "moe", "mof", "mog", "moh", "mok", "mol", "mom", "mon", "moo", "mop", "mor", "mos", "mot", "mou", "mov", "moz", "mp3", "mps", "muc", "mue", "muf", "mul", "mum", "mun", "mur", "mus", "mut", "myr", "mys", "myt", "nab", "nac", "nad", "nae", "nag", "nah", "nai", "nam", "nan", "nap", "nar", "nas", "nat", "nau", "nav", "naz", "nea", "neb", "nec", "nef", "neg", "neh", "nei", "nek", "nel", "nen", "neo", "nep", "ner", "nes", "net", "neu", "new", "nib", "nic", "nid", "nie", "nig", "nih", "nik", "nil", "nim", "nip", "nir", "nis", "nit", "niv", "nix", "niz", "nob", "noc", "noe", "nom", "non", "nop", "nor", "nos", "not", "nou", "nov", "now", "nua", "nud", "nue", "nuk", "nul", "num", "nus", "nut", "nva", "nyl", "nym", "oas", "obd", "obe", "obg", "obi", "obj", "obl", "obm", "obo", "obr", "obs", "obt", "obu", "obz", "och", "ock", "ode", "odi", "odo", "ody", "oed", "oef", "oeh", "oek", "oel", "oen", "oer", "oes", "ofe", "off", "ohn", "ohr", "okk", "okt", "oku", "okz", "ola", "old", "ole", "olf", "olg", "oli", "oll", "oly", "oma", "ome", "omi", "omn", "ona", "onk", "ont", "ony", "opa", "ope", "opf", "opi", "opo", "opp", "opt", "opu", "ora", "orb", "orc", "ord", "org", "ori", "ork", "orl", "orn", "ort", "osc", "osk", "osl", "osm", "osn", "osr", "oss", "ost", "osz", "oto", "ott", "out", "ouv", "ouz", "ova", "ove", "ovu", "owe", "oxi", "oxy", "oze", "ozo", "paa", "pac", "pad", "pae", "paf", "pag", "pak", "pal", "pam", "pan", "pap", "par", "pas", "pat", "pau", "pav", "paz", "pcs", "pda", "pec", "ped", "peg", "pei", "pej", "pek", "pel", "pen", "pep", "per", "pes", "pet", "pfa", "pfe", "pfi", "pfl", "pfo", "pfr", "pfu", "pha", "phe", "phi", "phl", "pho", "phr", "phy", "pia", "pic", "pie", "pig", "pik", "pil", "pim", "pin", "pio", "pip", "pir", "pis", "pit", "pix", "piz", "pkw", "pla", "ple", "pli", "plo", "plu", "pne", "poc", "pod", "poe", "pog", "poh", "poi", "pok", "pol", "pom", "pon", "poo", "pop", "por", "pos", "pot", "pow", "pra", "pre", "pri", "pro", "prs", "pru", "psa", "psc", "pse", "psy", "pub", "puc", "pud", "pue", "puf", "pul", "pum", "pun", "pup", "pur", "pus", "put", "puz", "pvc", "pyg", "pyj", "pyr", "pyt", "qua", "que", "qui", "quo", "rab", "rac", "rad", "rae", "raf", "rag", "rah", "rai", "rak", "ral", "ram", "ran", "rap", "rar", "ras", "rat", "rau", "rav", "raz", "rea", "reb", "rec", "red", "ree", "ref", "reg", "reh", "rei", "rej", "rek", "rel", "rem", "ren", "reo", "rep", "req", "res", "ret", "reu", "rev", "rex", "rez", "rha", "rhe", "rhi", "rho", "rhy", "rib", "ric", "rie", "rif", "rig", "rik", "ril", "rin", "rio", "rip", "ris", "rit", "riv", "riz", "roa", "rob", "roc", "rod", "roe", "rog", "roh", "rok", "rol", "rom", "ron", "ros", "rot", "rou", "row", "roy", "rtl", "rua", "rub", "ruc", "rud", "rue", "ruf", "ruh", "rui", "rum", "run", "rup", "rus", "rut", "rws", "ryb", "sa.", "saa", "sab", "sac", "sad", "sae", "saf", "sag", "sah", "sai", "sak", "sal", "sam", "san", "sap", "sar", "sas", "sat", "sau", "sav", "sax", "sca", "sce", "sch", "sci", "scr", "sea", "seb", "sec", "sed", "see", "seg", "seh", "sei", "sek", "sel", "sem", "sen", "seo", "sep", "seq", "ser", "ses", "set", "seu", "sex", "sez", "sha", "she", "shi", "sho", "shr", "shu", "sia", "sib", "sic", "sid", "sie", "sig", "sik", "sil", "sim", "sin", "sip", "sir", "sis", "sit", "ska", "ske", "ski", "skl", "sko", "skr", "sku", "sky", "sla", "sli", "slo", "slu", "sma", "smo", "sna", "sni", "sno", "so.", "soa", "soc", "sod", "soe", "sof", "sog", "soh", "soj", "sol", "som", "son", "sop", "sor", "sos", "sot", "sou", "sow", "soz", "spa", "spe", "sph", "spi", "spl", "spo", "spr", "spu", "squ", "sta", "ste", "sti", "sto", "str", "stu", "sty", "sub", "suc", "sud", "sue", "suf", "sug", "suh", "sui", "suj", "suk", "sul", "sum", "sup", "sur", "sus", "sut", "sve", "swe", "swi", "syl", "sym", "syn", "syr", "sys", "sze", "tab", "tac", "tad", "tae", "taf", "tag", "tai", "tak", "tal", "tam", "tan", "tao", "tap", "tar", "tas", "tat", "tau", "tav", "tax", "tay", "tby", "tea", "tec", "ted", "tee", "teg", "teh", "tei", "tek", "tel", "tem", "ten", "tep", "ter", "tes", "teu", "tex", "tha", "the", "thi", "tho", "thr", "thu", "thy", "tib", "tic", "tid", "tie", "tig", "til", "tim", "tin", "tip", "tir", "tis", "tit", "toa", "tob", "toc", "tod", "toe", "toh", "toi", "tok", "tol", "tom", "ton", "too", "top", "tor", "tos", "tot", "tou", "tox", "toy", "tra", "tre", "tri", "tro", "tru", "tsa", "tsc", "tse", "tub", "tuc", "tue", "tug", "tul", "tum", "tun", "tup", "tur", "tus", "tut", "tvs", "twi", "tyc", "typ", "tyr", "udo", "ueb", "uel", "uep", "uer", "ufe", "uff", "ufr", "uhr", "uhu", "ukr", "uku", "ulf", "uli", "ulk", "ulm", "ulr", "ult", "uma", "umb", "umc", "umd", "ume", "umf", "umg", "umh", "umi", "umj", "umk", "uml", "umm", "umn", "umo", "ump", "umq", "umr", "ums", "umt", "umv", "umw", "umz", "una", "unb", "unc", "und", "une", "unf", "ung", "unh", "uni", "unk", "unl", "unm", "unn", "uno", "unp", "unq", "unr", "uns", "unt", "unu", "unv", "unw", "unz", "upd", "upg", "ura", "urb", "urd", "ure", "urf", "urg", "urh", "uri", "urk", "url", "urm", "urn", "uro", "urp", "urs", "urt", "uru", "urv", "urw", "urz", "usa", "usb", "use", "usi", "usu", "uta", "ute", "uti", "uto", "uwe", "uze", "vae", "vag", "vak", "val", "vam", "van", "var", "vas", "vat", "veg", "veh", "vei", "vek", "ven", "ver", "ves", "vet", "via", "vib", "vic", "vid", "vie", "vik", "vil", "vio", "vip", "vir", "vis", "vit", "viz", "vla", "voe", "vog", "voi", "vok", "vol", "von", "vor", "vos", "vot", "voy", "vul", "vws", "waa", "wab", "wac", "wad", "wae", "waf", "wag", "wah", "wai", "wal", "wam", "wan", "wap", "war", "was", "wat", "way", "wcs", "web", "wec", "wed", "weg", "weh", "wei", "wel", "wen", "wer", "wes", "wet", "wgs", "whi", "wic", "wid", "wie", "wik", "wil", "wim", "win", "wip", "wir", "wis", "wit", "wla", "wms", "wob", "woc", "wod", "woe", "wog", "woh", "wok", "wol", "won", "wor", "wra", "wri", "wuc", "wue", "wul", "wun", "wup", "wur", "wus", "wut", "wyn", "x-b", "xan", "xeo", "xyl", "yac", "yan", "yeb", "yen", "yet", "yog", "yor", "yuc", "yup", "yvo", "zac", "zae", "zag", "zah", "zan", "zap", "zar", "zas", "zau", "zeb", "zec", "zed", "zeh", "zei", "zel", "zem", "zen", "zep", "zer", "zet", "zeu", "zic", "zie", "zif", "zig", "zik", "zim", "zin", "zio", "zip", "zir", "zis", "zit", "ziv", "zlo", "zob", "zoc", "zoe", "zof", "zog", "zol", "zom", "zon", "zoo", "zop", "zor", "zot", "zua", "zub", "zuc", "zud", "zue", "zuf", "zug", "zuh", "zui", "zuj", "zuk", "zul", "zum", "zun", "zuo", "zup", "zuq", "zur", "zus", "zut", "zuv", "zuw", "zuz", "zwa", "zwe", "zwi", "zwo", "zya", "zyk", "zyl", "zyn", "zyp", "zys"])

		got_lemma = False
		for l in range(len(word)):
			word_no_prefix = word[l:]
			prefix = word[:l]
			if word_no_prefix[:3] in alphabet:
				for lemma, wordforms in lexicon[word_no_prefix[:3]].iteritems():
					if word_no_prefix in tuple(wordforms):
						got_lemma = True
						return prefix+lemma

		if not got_lemma:
			return word



class NormalizerRU(object):

	
	def normalizeE(self, word):
		"""
		–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–π —ë --> –µ
		"""

		if '—ë' in word:
			word = word.replace('—ë', '–µ')
				
		return word



class NormalizerEN(object):


	def del_contractions(self, str1):
		"""
		–ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤ –ø—É—Ç–µ–º –º–µ—Ç–æ–¥–∞ re.sub (–∑–∞–º–µ–Ω—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ –Ω–∞ –Ω–∏—á—Ç–æ).
		–ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤—Å—ë –µ—â—ë –Ω–∞–π–¥–µ–Ω —É —Å–ª–æ–≤–∞ (I'd've), —Ç–æ –µ—â—ë —Ä–∞–∑ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ re.sub.
		"""

		# –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ —Å–ª–æ–≤ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–∏–ø–∞ you've, don't –∏ –ø—Ä.
		del_endings = re.compile(r'[\'\‚Äô]+[s|m|t|d|n]$|[\'\‚Äô](ve|ll|re|nt|ya|yer)$')

		no_endings = del_endings.sub('', str1)

		if del_endings.search(no_endings):

			no_endings2 = del_endings.sub('', no_endings)
			
			return no_endings2
		else:

			return no_endings

	def token_transform(self, token, irreg_verbs, irreg_nouns):
		"""
		–§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã –∏ —Ñ–æ—Ä–º—É –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–Ω.—á. —Å—É—â-—Ö.
		–°–º–æ—Ç—Ä–∏—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω –∫–ª—é—á–æ–º –≤ —Å–ª–æ–≤–∞—Ä—è—Ö irreg_verbs –∏ irreg_nouns,
		–µ—Å–ª–∏ —è–≤–ª—è–µ—Ç—Å—è, —Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–¥–∏—Ç –≤ value —Å–ª–æ–≤–∞—Ä–µ–π.
		"""

		if token in irreg_verbs:
			return irreg_verbs[token]
		elif token in irreg_nouns:
			return irreg_nouns[token]
		else:
			return token



class BuildTermSpace(object):

	"""
	–°–æ–∑–¥–∞–Ω–∏–µ json-–æ–±—ä–µ–∫—Ç–∞ –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è, –≤ –∫–æ—Ç–æ—Ä–æ–º —Ö—Ä–∞–Ω—è—Ç—Å—è —Å—Ç–µ–º–º—ã
	–∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ –∏ –∏—Ö —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤.
	"""

	def __init__(self, language='en', action='tfidf'):

		# –í—ã–∑—ã–≤–∞–µ–º LoadExternalLists, —Å–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤, 
		# –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–º–µ—Ü–∫–∏–π –ª–µ–∫—Å–∏–∫–æ–Ω,
		# 
		self.language = language
		self.action = action

		# –∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —É–¥–∞–ª—è—Ç—å—Å—è –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —Ç–æ–∫–µ–Ω–∞
		self.punctuation = "‚àô!‚Äº¬°\"#¬£‚Ç¨$¬•%&'()*+¬±√ó√∑¬∑,-./:;<=>?¬ø@[\]^ÀÜ¬®_`‚Äî‚Äì¬≠{|}~‚âà‚â†‚Üí‚Üì¬¨‚Äô‚Äú‚Äù¬´¬ª‚â´‚Äò‚Ä¶¬¶‚Ä∫üåº‚Ä≤‚Ä≥¬π¬ß¬º‚Öú¬Ω¬æ‚Öò¬©‚úí‚Ä¢‚ñ∫‚óè‚òÖ‚ù§‚û°‚ûú‚ûö‚ûò‚ûî‚úî‚ûì‚ûí‚ûë‚ûê‚ûè‚ûé‚ûç‚ûå‚ûã‚ûä‚ù∏‚ù∑‚ñ†‚Ä†‚úù‚úåÔøºÔ∏è¬≥‚Äé¬≤‚Äö‚Äû ‚Äã"

		loadRes = LoadExternalLists()

		if self.language == 'de':
			self.stopwords = loadRes.loadStopWordsDE()
			# –æ–±—ä–µ–∫—Ç —Å—Ç–µ–º–º–µ—Ä–∞
			self.stemmer = GermanStemmer()
			# –Ω–µ–º–µ—Ü–∫–∏–π —Å–ª–æ–≤–∞—Ä—å
			print '\n', "Loading German Dictionary... OK", '\n'
			self.lexicon_de = loadRes.loadLexiconDe()
			self.normalizer = NormalizerDE()
		elif self.language == 'ru':
			self.stopwords = loadRes.loadStopWordsRU()
			self.stemmer = RussianStemmer()
			# –æ–±—ä–µ–∫—Ç pymorphy2.MorphAnalyzer(), –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç normal_form
			self.lemmatizer_ru = pymorphy2.MorphAnalyzer()
			self.normalizer = NormalizerRU()
		else:
			self.stopwords = loadRes.loadStopWordsEN()
			self.stemmer = PorterStemmer()
			self.normalizer = NormalizerEN()
			# —Å–ø–∏—Å–æ–∫ –Ω–µ–ø—Ä–∞–≤. –≥–ª.
			self.irreg_verbs = loadRes.loadVerbForms()
			# —Å–ø–∏—Å–æ–∫ –Ω–µ–ø—Ä–∞–≤. —Å—É—â-—Ö
			self.irreg_nouns = loadRes.loadNounforms()
	

	def processString(self, line):
		"""
		–§—É–Ω–∫—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞. –ü–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å—Ç—Ä–æ–∫—É, —Å–æ–∑–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ tokens,
		—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ç—É–¥–∞ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ re.split'–æ–º —Å–ª–æ–≤–∞, '–æ—Ç—Ä–µ–∑–∞—è' –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é —Å –∫–æ–Ω—Ü–æ–≤ —Å–ª–æ–≤–∞ –∏ –ø–æ–Ω–∏–∂–∞—è —Ä–µ–≥–∏—Å—Ç—Ä, 
		–∏ —É–¥–∞–ª—è–µ—Ç –ø–æ —Ö–æ–¥—É –æ–∫–æ–Ω—á–∞–Ω–∏—è-—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–µ–π del_contractions
		–î–∞–ª—å—à–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∫ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é —Å–ø–∏—Å–∫–∞ —Å—Ç–µ–º–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ rslt_list —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤
		–∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.
		–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ rslt_list, –≤ –∫–æ—Ç–æ—Ä–æ–º —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å—Ç–µ–º–º—ã –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤.
		"""

		# –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ —Å–ª–µ—à–∞–º
		splitchars = re.compile(r'[\s\\\/\(\)\[\]\<\>\;\:\,\‚Äö\‚Äî\?\!\|\"¬´¬ª‚Ä¶#]|\.\.\.+|[‚ÄÇÔøΩ‚åÇ¬†‚àû¬Ω¬æ‚ñ∫=]|\-\-|\.[\'\"‚Äô‚Äú‚Äù¬´¬ª‚Äò‚Ä≤‚Ä≥‚Äû-]') # [\.\:][\'\"‚Äô‚Äú‚Äù¬´¬ª‚Äò‚Ä≤‚Ä≥]

		# –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ü–∏—Ñ—Ä—ã
		esc_num = re.compile(r'[0-9]+')

		# –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è URL
		#url_esc = re.compile(r'([a-z]{3,6}:\/\/)?([a-zA-Z0-9\-@?]+[\.|\:])+[a-z]{2,13}[\.\?\=\&\%\,\#\+\(\)\/\w\-]*')

				
		if self.language == 'de':
			tokens = (self.normalizer.normalizeUmlaut(self.normalizer.deleteContrs(token.strip(self.punctuation).lower())) for token in splitchars.split(line))
			rslt_list = (self.stemmer.stem(self.normalizer.lemmatize(term, self.lexicon_de)) for term in tokens if term not in self.stopwords and not esc_num.search(term) and len(term)>0)	# and not esc_num.search(term) - –≤–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ —É—Å–ª–æ–∏—è –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª—è—Ç—å —Ç–æ–∫–µ–Ω—ã —Å —Ü–∏—Ñ—Ä–∞–º–∏

		elif self.language == 'ru':
			tokens = (self.normalizer.normalizeE(token.strip(self.punctuation).lower()) for token in splitchars.split(line))
			rslt_list = (self.stemmer.stem(self.lemmatizer_ru.parse(term)[0].normal_form) for term in tokens if term not in self.stopwords and not esc_num.search(term) and len(term)>0)	# and not esc_num.search(term) - –≤–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ —É—Å–ª–æ–∏—è –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª—è—Ç—å —Ç–æ–∫–µ–Ω—ã —Å —Ü–∏—Ñ—Ä–∞–º–∏

		else:
			# –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: –ø–æ —Ü–∏–∫–ª—É: —Ä–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ regexp splitchars,
			# 2. —É–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –≤–æ–∫—Ä—É–≥ —Ç–æ–∫–µ–Ω–∞, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, 
			# 3. —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ñ–æ—Ä–º—É –Ω–µ–ø—Ä–∞–≤. –≥–ª–∞–≥. –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é
			# 4. —É–¥–∞–ª—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è-—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Å \'
			tokens = (self.normalizer.token_transform(self.normalizer.del_contractions(token.strip(self.punctuation).lower()), self.irreg_verbs, self.irreg_nouns) for token in splitchars.split(line))

			# –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ø–∏—Å–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤: –µ—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω –Ω–µ –≤ —Å–ø–∏—Å–∫–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä, —Ç–æ —Å—Ç–µ–º–º–∏—Ä—É–µ–º –µ–≥–æ.
			rslt_list = (self.stemmer.stem(term, 0, len(term)-1) for term in tokens if term not in self.stopwords and not esc_num.search(term) and len(term)>0)	# and not esc_num.search(term) - –≤–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ —É—Å–ª–æ–∏—è –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª—è—Ç—å —Ç–æ–∫–µ–Ω—ã —Å —Ü–∏—Ñ—Ä–∞–º–∏
		

		if not rslt_list:
			return []

		else:
			return rslt_list


	def processFile(self, filename):
		"""
		–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –≤ utf-16, –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ñ–∞–π–ª–∞ –≤—ã–∑—ã–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é processString,
		–∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–∑ –ø–æ–ª—É—á–∏–≤—à–µ–≥–æ—Å—è —Å–ø–∏—Å–∫–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ set terms_set, –∏–∑–±–∞–≤–ª—è—è—Å—å –æ—Ç
		–¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
		–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º.
		"""

		terms_set = set()
		terms_list = []

		if self.action == 'tfidf':
			try:
				with codecs.open(filename, 'r', 'utf-16') as infile:
					
					for line in infile:
						if len(line) > 1:
							for term in self.processString(line):
								terms_set.add(term)
								
			except (UnicodeDecodeError, UnicodeError, IOError):
				pass

			return terms_set

		if self.action == 'raw':
			try:
				with codecs.open(filename, 'r', 'utf-16') as infile:
					
					for line in infile:
						if len(line) > 1:
							for term in self.processString(line):
								terms_list.append(term)
								
			except (UnicodeDecodeError, UnicodeError, IOError):
				pass

			return terms_list


	def crawl(self, dirname):
		"""
		–§—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –ø–∞–ø–∫–∞–º –∏ –ø–æ–¥–ø–∞–ø–∫–∞–º —É–∫–∞–∑–∞–Ω–Ω–æ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
		–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å–ª–∏ —Ñ–∞–π–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–π, —Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é processFile –∏ —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
		–µ—ë —Ä–∞–±–æ—Ç—ã –≤ set terms_set.
		–í –æ–±—â–µ–º terms_dict –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –ª–µ–º–º—ã, —Å–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ json.
		terms_dict –æ—Ç—Ä–∞–∂–∞–µ—Ç –ø–æ —Å—É—Ç–∏ –≤—Ç–æ—Ä—É—é —á–∞—Å—Ç—å —Ñ–æ—Ä–º—É–ª—ã tfidf, —Ç.–µ. –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤ –∫–∞–∫–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
		–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è —Ç–µ—Ä–º–∏–Ω.
		"""

		docs_num = 0

		terms_dict = defaultdict(int)
		
		for root, dirs, files in os.walk(dirname):

			print root, "processing..."
			
			for filename in files:

				if filename.endswith('.txt') or filename.endswith('.TXT'):
					
					print filename

					terms_set = self.processFile(join(root,filename))

					for term in terms_set:

						terms_dict[term] += 1

					docs_num+=1
					
		if self.action == 'raw':
			with codecs.open(r'.\termSpace\\'+self.language.upper()+'frequency_list_stem.txt', 'w', 'utf-16') as outfile:
				for key, value in sorted(terms_dict.iteritems(), key=lambda x:x[1], reverse=True):
					outfile.write(key+'\t'+str(value))
					outfile.write('\n')
		
		if self.action == 'tfidf':

			with open(r".\termSpace\\" + self.language.upper() + "CorpusDict_" + str(docs_num) + ".json", 'w') as  outfile:
				json.dump(terms_dict, outfile)

		



def main():

	if len(sys.argv) != 4:
		print '\n', 'Sctipt requires parameters!','\n'
		print 'Usage: [script.py] [path_to_corpus] [en | de | ru] [tfidf | raw]'
		sys.exit(1)

	dir_path = sys.argv[1]
	language = sys.argv[2]
	# action = 1) tfidf = count stems for tfidf, 2) raw = count absolute freq. of each stem
	action = sys.argv[3]

	trms = BuildTermSpace(language, action)
	trms.crawl(dir_path)


if __name__ == '__main__':
	main()
